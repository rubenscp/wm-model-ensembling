# This module aims to implement the model ensembling method for the previous models 
# explored in our research as SSD, Faster RCNN, YOLO series, DETR, and TransUNEt models.
#  
# The code below was generated by CHatGPT with the following prompt:
#   "Give me an pytorch example using model ensembling for object detection with the pretrained models SSD, Faster RCNN, YOLOv8, YOLOv9, YOLOv10, 
#   DETR, and TRansUNet (adapated for obejct detection)"
# 

import math 
import torch 
import torchvision
from torchvision.ops import * 

from common.manage_log import *

def run_ensemble_test_images(parameters, all_predictions):
    
    print(f'len(all_predictions): {len(all_predictions)}')
    print(f'all_predictions: {all_predictions}')

    # getting all predictions keys as test image filename for all inferenced models 
    test_image_filenames = get_all_test_image_filenames(all_predictions)            
    print(f'len(test_image_filenames): {len(test_image_filenames)}')
    print(f'test_image_filenames: {test_image_filenames}')

    # getting IoU threshold 
    iou_threshold = parameters['neural_network_model']['iou_threshold']
    iou_threshold_for_nms = parameters['neural_network_model']['iou_threshold_for_nms']

    # count = 0

    # ensembling the inference results for each test image 
    for test_image_filename in test_image_filenames:

        # count += 1
        # if count > 50:
        #     break

        print(f'')
        print(f'----------------------------------------------------')
        print(f'test_image_filename: {test_image_filename}')

        # flattening list of predictions
        print(f'')
        print(f'get_flattened_predictons')
        flattened_predictions = get_flattened_predictons(all_predictions, test_image_filename)
        all_predictions_size = len(flattened_predictions)

        print(f'len(flattened_predictions): {len(flattened_predictions)}')
        print(f'flattened_predictions: {flattened_predictions}')

        # grouping detections based on overlapping of their bounding boxes and classes
        # by using the IoU metric
        print(f'')
        print(f'get_grouped_predictions')
        grouped_predictions = get_grouped_predictions(
            flattened_predictions, parameters['neural_network_model']['iou_threshold']
        )
        print(f'len(grouped_predictions): {len(grouped_predictions)}')
        i = 1
        for grouped_predicton in grouped_predictions:
            print(f'grouped_predicton: {i}) {len(grouped_predicton)} - {grouped_predicton}')
            i += 1

        # applying voting strategies in the grouped predictions: affirmative, consensus, and unanimous
        affirmative_predictions, consensus_predictions, unanimous_predictions = \
            apply_voting_strategies(grouped_predictions, all_predictions_size)
        print(f'')
        print(f'Voting Strategies Results:')
        print(f'')
        print(f'affirmative_predictions: {len(affirmative_predictions)} - {affirmative_predictions}')
        print(f'consensus_predictions: {len(consensus_predictions)} - {consensus_predictions}')
        print(f'unanimous_predictions: {len(unanimous_predictions)} - {unanimous_predictions}')

        # applying non-maximumk supress (nms) in the predictions 
        affirmative_nms_predictions = apply_nms_into_predictions(affirmative_predictions, iou_threshold)
        consensus_nms_predictions = apply_nms_into_predictions(consensus_predictions, iou_threshold)
        unanimous_nms_predictions = apply_nms_into_predictions(unanimous_predictions, iou_threshold)

        print(f'')
        print(f'affirmative_predictions after nms: {len(affirmative_predictions)} - {affirmative_predictions}')
        print(f'consensus_predictions after nms: {len(consensus_predictions)} - {consensus_predictions}')
        print(f'unanimous_predictions after nms: {len(unanimous_predictions)} - {unanimous_predictions}')



    print(f'Stop on exit()')
    exit()






def get_all_test_image_filenames(predictions):

    # getting all test image filenames with no duplicates
    test_image_filenames = {}
    for model in list(predictions.keys()):
        for image_filename in list(predictions[model].keys()):
            test_image_filenames[image_filename] = image_filename
                
    # returning test image filenames 
    return test_image_filenames


def get_flattened_predictons(predictions, test_image_filename):

    # creating flattened predictions of one image from all models 
    flattened_predictions = []

    for model in list(predictions.keys()):
        if len(predictions[model]) > 0:
            test_image_predictions = predictions[model][test_image_filename]
            # print(f'test_image_filename: {test_image_filename}')
            # print(f'model: {model}')
            # print(f'test_image_predictions: {len(test_image_predictions)} - {test_image_predictions}')

            # get all valid predictions of the model 
            image_predictions = test_image_predictions[1]
            # print(f'image_predictions: {image_predictions}')
            if len(image_predictions) == 0: 
                continue 

            # flattening image predictions 
            for box, score, label in zip(image_predictions[0]['boxes'], 
                                             image_predictions[0]['scores'], 
                                             image_predictions[0]['labels']):
                # setting data of the one detection 
                detection = {}
                detection['box'] = box 
                detection['score'] = score
                detection['label'] = label
                detection['model'] = model
                
                # adding detection to flattened predictions 
                flattened_predictions.append(detection)

    # returning flattened predictions
    return flattened_predictions

# grouping detections based on overlapping of their bounding boxes and classes
# by using the IoU metric
def get_grouped_predictions(flattened_predictions, iou_threshold):

    # print(f'flattened_predictions: {len(flattened_predictions)} - {flattened_predictions}')
    # print(f'iou_threshold: {iou_threshold}')

    # setting flattened predictions as NOT removed
    for prediction in flattened_predictions:
        prediction['removed'] = False        

    # creating grouped predictions of one image from flattened predictions 
    grouped_predictions = []
    one_grouped_prediction = []

    # calculating the IoU between all predictions of the same class for grouping 
    for i in range(len(flattened_predictions)):
        # getting bounding box reference to compare with the others
        bounding_box_reference = flattened_predictions[i]
        if bounding_box_reference['removed']:
            continue

        # setting one grouped prediction with first prediction (reference)
        one_grouped_prediction.append(bounding_box_reference)

        # removing bounding box reference from the flatted predictions 
        bounding_box_reference['removed'] = True

        # comparing with others bounding boxes 
        for j in range(i+1, len(flattened_predictions)):
            # getting bounding box for comparision 
            bounding_box_next = flattened_predictions[j]
            if bounding_box_next['removed']:
                continue

            # calculating IoU of the two bounding boxes: reference and next
            # Both sets of boxes are expected to be in (x1, y1, x2, y2)
            box_reference = bounding_box_reference['box']
            box_next = bounding_box_next['box']
            iou = box_iou(box_reference.unsqueeze(0), box_next.unsqueeze(0))

            # evaluating overlapping of bounding boxes reference and next
            if (bounding_box_reference['label'] ==  bounding_box_next['label']) and \
               (iou >= iou_threshold):

                # setting IoU value 
                bounding_box_next['iou'] = iou

                # removing bounding box reference from the flatted predictions 
                bounding_box_next['removed'] = True

                # print(f'bbox same class - iou: {iou}')
                # print(f'bbox same class: ref:{bounding_box_reference}')
                # print(f'bbox same class: next:{bounding_box_next}')
                one_grouped_prediction.append(bounding_box_next)

        # adding one new grouped prediction
        grouped_predictions.append(one_grouped_prediction)

        # initializing one new grouped prediction
        one_grouped_prediction = []

    # returning grouped predictions         
    return grouped_predictions

# applying voting strategies in the grouped predictions that can be of three strategies:
# 1) affirmative: all grouped predictions 
# 2) consensus: the group size must be greater than m/2, where m is the size of flattened predeictions
# 3) unanimous: the group size must be equal to m size
def apply_voting_strategies(grouped_predictions, all_predictions_size):

    # creating results list for each strategy
    affirmative_predictions = []
    consensus_predictions = []
    unanimous_predictions = []

    # processing all grouped predictions
    for grouped_prediction in grouped_predictions:

        # affirmative strategy
        for prediction in grouped_prediction:
            affirmative_predictions.append(prediction)

        # consensus strategy
        if len(grouped_prediction) >= math.ceil(all_predictions_size / 2.0):
            for prediction in grouped_prediction:
                consensus_predictions.append(prediction)

        # unanimous strategy 
        if len(grouped_prediction) == all_predictions_size:
            for prediction in grouped_prediction:
                unanimous_predictions.append(prediction)

    # returning results of ensembling 
    return affirmative_predictions, consensus_predictions, unanimous_predictions


def apply_nms_into_predictions(predictions, iou_threshold):
    
    # initializing kept prediction 
    kept_prediction = {}

    # evaluating predictions size
    if len(predictions) == 0:
        print(f'predictions list for nms is empty')
        return kept_prediction
    
    print(f'')
    print(f'predictions for nms: {predictions}')

    # preparing boxes and scores to apply nms
    bounding_boxes = []
    scores = []
    for prediction in predictions:
        bounding_boxes.append(prediction['box'].numpy())
        scores.append(prediction['score']) 

    bounding_boxes = torch.Tensor(bounding_boxes)        
    scores = torch.Tensor(scores)
    print(f'bounding boxes shape  : {bounding_boxes.shape}')
    print(f'bounding boxes for nms: {bounding_boxes}')
    print(f'scores shape  : {scores.shape}')
    print(f'scores for nms: {scores}')
    print(f'')
    
    # applying nms in the predictions
    keep_index = nms(bounding_boxes, scores, iou_threshold)
    print(f'keep_index after nms: {keep_index}')

    # preparing kept predictions 
    kept_predictions = []
    for i in range(len(predictions)):
        if i in keep_index:
            kept_predictions.append(predictions[i])

    # kept_predictions = predictions[keep_index]
    print(f'prediction after nms: {kept_predictions}')
    return kept_predictions


# # grouping detections based on overlapping of their bounding boxes and classes
# # by using the IoU metric
# def get_grouped_predictions(flattened_predictions, iou_threshold):

#     print(f'flattened_predictions: {len(flattened_predictions)} - {flattened_predictions}')
#     print(f'iou_threshold: {iou_threshold}')

#     # setting flattened predictions as NOT removed
#     # for prediction in flattened_predictions:
#     #     prediction['removed'] = False        
#     # print(f'flattened_predictions: {len(flattened_predictions)} - {flattened_predictions}')

#     # for prediction in flattened_predictions:
#     #     prediction['box'] = prediction['box'].numpy()
#     #     prediction['score'] = prediction['score'].numpy()
#     #     prediction['label'] = prediction['label'].numpy()
#     print(f'flattened_predictions: {len(flattened_predictions)} - {flattened_predictions}')
    
#     # creating grouped predictions of one image from flattened predictions 
#     grouped_predictions = []
#     one_grouped_prediction = []

#     # calculating the IoU between all predictions of the same class for grouping
#     flattened_predictions_size = len(flattened_predictions)
#     while (flattened_predictions_size > 0):
    
#         # getting bounding box reference to compare with the others
#         bounding_box_reference = flattened_predictions[0]
#         one_grouped_prediction.append(bounding_box_reference)

#         # removing bounding box reference from the flatted predictions 
#         flattened_predictions.remove(bounding_box_reference)
#         # bounding_box_reference['removed'] = True

#         # getting all next predictions 
#         # next_flattened_predictions = flattened_predictions[1:]
#         next_flattened_predictions = flattened_predictions.copy()
#         print(f'flattened_predictions after remove reference     : {len(flattened_predictions)} - {flattened_predictions}')
#         print(f'next_flattened_predictions after remove reference: {len(next_flattened_predictions)} - {next_flattened_predictions}')

#         # comparing with others bounding boxes
#         for bounding_box_next in next_flattened_predictions:

#             # calculating IoU of the two bounding boxes: reference and next
#             # Both sets of boxes are expected to be in (x1, y1, x2, y2)
#             box_reference = bounding_box_reference['box']
#             box_next = bounding_box_next['box']
#             iou = box_iou(box_reference.unsqueeze(0), box_next.unsqueeze(0))

#             # print(f'box_reference: {box_reference}')
#             # print(f'box_next: {box_next}')
#             # print(f'iou: {iou}')

#             # evaluating overlapping of bounding boxes reference and next
#             if (bounding_box_reference['label'] ==  bounding_box_next['label']) and \
#                (iou >= iou_threshold):

#                 print(f'bbox same class: ref:{bounding_box_reference}')
#                 print(f'bbox same class: next:{bounding_box_next}')
#                 print(f'bbox same class - iou: {iou}')
#                 # bounding_box_next = bounding_box_next.copy()
#                 one_grouped_prediction.append(bounding_box_next)

#                 # removing bounding box next from the flatted predictions
#                 print(f'flattened_predictions to remove next: {len(flattened_predictions)} - {flattened_predictions}')
#                 print(f'bounding_box_next: {bounding_box_next}')
#                 flattened_predictions.remove(bounding_box_next)
#                 # for prediction in flattened_predictions:
#                 #     if torch.equal(prediction, bounding_box_next):  # Ensure equality comparison
#                 #         flattened_predictions.remove(prediction)
#                 #         break
#                 print(f'flattened_predictions after remove next: {len(flattened_predictions)} - {flattened_predictions}')

#             # adding one new grouped prediction
#             grouped_predictions.append(one_grouped_prediction)

#             # initializing one new grouped prediction
#             one_grouped_prediction = []

#             # getting size of the flattened predictions
#             flattened_predictions_size = len(flattened_predictions)

#     # returning grouped predictions         
#     return grouped_predictions

# # grouping detections based on overlapping of their bounding boxes and classes
# # by using the IoU metric
# def get_grouped_predictions(flattened_predictions, iou_threshold):

#     print(f'flattened_predictions: {flattened_predictions}')
#     print(f'iou_threshold: {iou_threshold}')

#     # creating grouped predictions of one image from flattened predictions 
#     grouped_predictions = []
#     one_grouped_prediction = []

#     # calculating the IoU between all predictions of the same class for grouping 
#     for i in range(len(flattened_predictions)):
#         # getting bounding box reference to compare with the others
#         bounding_box_reference = flattened_predictions[i]
#         one_grouped_prediction.append(bounding_box_reference)

#         # comparing with others bounding boxes 
#         for j in range(i+1, len(flattened_predictions)):
#             # getting bounding box for comparision 
#             bounding_box_next = flattened_predictions[j]

#             # calculating IoU of the two bounding boxes: reference and next
#             # Both sets of boxes are expected to be in (x1, y1, x2, y2)
#             box_reference = bounding_box_reference['box']
#             box_next = bounding_box_next['box']
#             iou = box_iou(box_reference.unsqueeze(0), box_next.unsqueeze(0))
#             # print(f'box_reference: {box_reference}')
#             # print(f'box_next: {box_next}')
#             # print(f'iou: {iou}')

#             # evaluating overlapping of bounding boxes reference and next
#             if (bounding_box_reference['label'] ==  bounding_box_next['label']) and \
#                (iou >= iou_threshold):
#                 print(f'bbox same class - iou: {iou}')
#                 print(f'bbox same class: ref:{bounding_box_reference}')
#                 print(f'bbox same class: next:{bounding_box_next}')
#                 one_grouped_prediction.append(bounding_box_next)

#             # adding one new grouped prediction
#             grouped_predictions.append(one_grouped_prediction)

#             # initializing one new grouped prediction
#             one_grouped_prediction = []


#     # returning grouped predictions         
#     return grouped_predictions



















# # Helper function for ensembling predictions
# def ensemble_predictions(predictions, iou_thresh=0.5):
#     """
#     Perform ensemble by combining predictions from multiple models.
#     - predictions: List of dicts with 'boxes', 'scores', 'labels' for each model
#     """
#     final_boxes = []
#     final_scores = []
#     final_labels = []

#     for pred in predictions:
#         final_boxes.append(pred['boxes'])
#         final_scores.append(pred['scores'])
#         final_labels.append(pred['labels'])

#     final_boxes = torch.cat(final_boxes)
#     final_scores = torch.cat(final_scores)
#     final_labels = torch.cat(final_labels)

#     # Non-Maximum Suppression (NMS) for final ensembling
#     keep = torchvision.ops.nms(final_boxes, final_scores, iou_thresh)
#     return final_boxes[keep], final_scores[keep], final_labels[keep]

# # Perform inference and ensemble predictions
# def run_ensemble(image):
#     # Prepare input
#     if isinstance(image, str):
#         from PIL import Image
#         image = Image.open(image).convert("RGB")
#     image_tensor = torchvision.transforms.ToTensor()(image).unsqueeze(0)

#     # Get predictions
#     predictions = []

#     with torch.no_grad():
#         predictions.append(ssd(image_tensor)[0])
#         predictions.append(faster_rcnn(image_tensor)[0])
#         detr_out = detr(image_tensor)[0]
#         predictions.append({
#             'boxes': detr_out['boxes'],
#             'scores': detr_out['scores'],
#             'labels': detr_out['labels']
#         })
#         predictions.append(yolo_v8.predict(image, verbose=False)[0])
#         predictions.append(yolo_v9.predict(image, verbose=False)[0])
#         predictions.append(yolo_v10.predict(image, verbose=False)[0])
#         predictions.append(transunet(image_tensor)[0])

#     # Ensemble the predictions
#     boxes, scores, labels = ensemble_predictions(predictions)
#     return boxes, scores, labels    


# # Example Usage
# if __name__ == "__main__":
#     import matplotlib.pyplot as plt
#     from PIL import Image
#     import random

#     # Load an example image
#     image_path = "example.jpg"  # Replace with your image path
#     image = Image.open(image_path).convert("RGB")

#     # Run ensemble
#     boxes, scores, labels = run_ensemble(image)

#     # Visualize results
#     plt.figure(figsize=(10, 10))
#     plt.imshow(image)
#     ax = plt.gca()

#     for box, score, label in zip(boxes, scores, labels):
#         xmin, ymin, xmax, ymax = box.tolist()
#         rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
#                               linewidth=2, edgecolor='red', facecolor='none')
#         ax.add_patch(rect)
#         ax.text(xmin, ymin - 10, f"Label: {label}, Score: {score:.2f}",
#                 color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))

#     plt.axis('off')
#     plt.show()    